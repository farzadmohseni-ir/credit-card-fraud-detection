# 💳 Credit Card Fraud Detection

An **Ensemble-Based Machine Learning** approach for detecting credit card fraud, enhanced by advanced **data balancing techniques** to handle class imbalance challenges effectively.
### 🏷️ **موضوع: کشف تقلب در کارت‌های اعتباری**

---

### 📌 **تعریف موضوع:**

کشف تقلب در کارت‌های اعتباری یکی از مسائل مهم در حوزه‌ی علم داده و امنیت مالی است. با گسترش استفاده از پرداخت‌های دیجیتال و تراکنش‌های آنلاین، شناسایی سریع و دقیق فعالیت‌های متقلبانه برای بانک‌ها، شرکت‌های پرداخت و نهادهای مالی ضروری است.

---

### 🎯 **اهداف تحقیق:**

1. طراحی و توسعه یک سامانهٔ هوشمند به‌منظور افزایش دقت در شناسایی تراکنش‌های تقلبی.  
2. کاهش نرخ تشخیص نادرست تراکنش‌های سالم به‌ ‌عنوان تقلب (کاهش مثبت‌های کاذب).  
3. افزایش نرخ شناسایی صحیح تراکنش‌های تقلبی به‌ منظور ارتقاء امنیت سامانه‌های پرداخت.  
4. جلوگیری از آسیب به اعتبار مؤسسات مالی در اثر تصمیمات نادرست سیستم‌های تشخیص تقلب.  
5. حفظ رضایت و اعتماد مشتریان با کاهش موارد رد نادرست تراکنش‌های معتبر.

---

### ⚠️ **چالش‌های اصلی:**

- عدم توازن شدید بین کلاس‌های «تقلب» و «عدم تقلب» در داده‌ها.  
- وجود نویز و ویژگی‌های نامربوط در دیتاست.  
- دشواری در ارزیابی مدل‌ها به دلیل نرخ پایین وقوع تقلب (rare event).  
- ریسک بیش‌برازش (Overfitting) در مدل‌های پیچیده.  
- نیاز به پیاده‌سازی تکنیک‌های پیش‌پردازش و انتخاب ویژگی.

---


### 📊 مقایسه سه دیتاست معروف مرتبط با کشف تقلب در کارت‌های اعتباری
<div dir="rtl">
  
| ویژگی                          | ULB 2016               | Nelgiriyewithana - 2023      | IEEE-CIS 2019                   |
|:-------------------------------|:-------------------------------------|:------------------------------------------|:---------------------------------------------|
| 🏷️ عنوان دیتاست               | Credit Card Fraud Detection         | Credit Card Fraud Detection Dataset 2023  | IEEE-CIS Fraud Detection                     |
| 🌍 منبع                        | ULB (Université Libre de Bruxelles) | Kaggle (کاربر nelgiriyewithana)           | IEEE-CIS / Vesta Corporation                 |
| 📦 تعداد رکورد                | 284,807                             | 550,000                                    | 590,540                                      |
| 🔢 تعداد ویژگی                 | 30                                  | 30                                         | 431 (400 عددی + 31 دسته‌ای)                 |
| 🧩 نوع ویژگی‌ها               | ناشناس‌شده با PCA (فقط V1-V28)     | ناشناس‌شده (Amount, Category, etc.)       | ویژگی‌های متنوع دستگاه، کارت، آدرس، ایمیل و ... |
| 🔍 نرخ تقلب                    | 0.172٪ (492 مورد)                  | شدیداً نامتوازن (تعداد دقیق مشخص نیست)   | 3.5٪ (20,663 مورد)                           |
| 🎯 نوع برچسب هدف              | 0 (عادی) و 1 (تقلب)                | 0 (عادی) و 1 (تقلب)                        | 0 (عادی) و 1 (تقلب)                          |
| ⚙️ کاربردپذیری برای تحقیق     | بله (مرجع معتبر علمی)             | بله، مناسب برای نوآوری                    | بله، بسیار مناسب برای رقابت و مدل‌های پیشرفته |
| 📚 پیشنهاد شده برای           | مقایسه مدل‌ها (Benchmark)          | نوآوری در مدل‌ها و ویژگی‌سازی             | یادگیری عمیق، مهندسی ویژگی، AutoML و ...     |

</div>

---

### 🧠 جدول مراحل آماده‌سازی داده و مدل‌سازی
<div dir="rtl">

| 🔢 ردیف | مرحله                 | عنوان انگلیسی                    | شرح مختصر                                                                 |
|--------|------------------------|----------------------------------|---------------------------------------------------------------------------|
| 1      | 📥 جمع‌آوری داده        | Data Collection                  | دریافت داده‌ها از منابع معتبر مانند دیتاست‌های عمومی یا پایگاه‌های سازمانی |
| 2      | 🧾 درک داده             | Data Understanding               | تحلیل اولیه برای شناخت ساختار، انواع ویژگی‌ها و برچسب‌ها                 |
| 3      | 🧹 پاک‌سازی داده        | Data Cleaning                    | اصلاح داده‌های گمشده، حذف داده‌های پرت، و رفع ناسازگاری‌ها              |
| 4      | 🛠️ پیش‌پردازش داده      | Data Preprocessing               | نرمال‌سازی، کدگذاری داده‌های دسته‌ای، و آماده‌سازی ویژگی‌ها             |
| 5      | ⚖️ متعادل‌سازی داده     | Data Balancing                   | رفع مشکل نامتوازنی کلاس‌ها                                               |
| 6      | 📊 تحلیل اکتشافی        | Exploratory Data Analysis (EDA)  | بررسی آماری ویژگی‌ها، رسم نمودارها و بررسی توزیع داده‌ها                |
| 7      | 🧪 مهندسی ویژگی         | Feature Engineering              | ساخت ویژگی‌های جدید یا تبدیل ویژگی‌های موجود برای بهبود مدل            |
| 8      | 🎯 انتخاب ویژگی         | Feature Selection                | حذف ویژگی‌های کم‌اثر و انتخاب ویژگی‌های کلیدی با تکنیک‌های آماری        |
| 9      | ✂️ تقسیم‌بندی داده       | Train-Test Split                 | جداسازی داده به مجموعه آموزش و آزمون برای ارزیابی مدل                    |
| 10     | 🤖 مدل‌سازی             | Modeling                         | آموزش مدل با الگوریتم‌های مناسب و تنظیم پارامترها                        |

</div>

---

### ⚖️ جدول نهایی روش‌های متعادل‌سازی داده (با مزایا، معایب، توضیح عملکرد و تناسب با دیتاست)

<div dir="rtl">

| 🔢 ردیف | 🧪 عنوان روش | 🧬 نوع | 🧩 توضیح روش | ✅ مزایا | ⚠️ معایب | 📊 مناسب برای دیتاست من؟ |
|--------|--------------|--------|--------------|----------|----------|----------------------------|
| 1 | Random Undersampling | Undersampling | با حذف تصادفی از کلاس غالب، کلاس‌ها را برابر می‌کند. | ساده و سریع، کاهش حجم داده | احتمال حذف اطلاعات مفید | ❌ خیلی ضعیف برای دیتاست بزرگ و نامتوازن |
| 2 | Random Oversampling | Oversampling | با تکرار تصادفی نمونه‌های کلاس اقلیت، تعادل ایجاد می‌کند. | حفظ داده‌ها، اجرای آسان | احتمال بیش‌برازش | ⚠️ فقط برای تست‌های اولیه یا مدل‌های ساده پیشنهاد می‌شود |
| 3 | SMOTE | Oversampling | داده‌های مصنوعی بین نزدیک‌ترین همسایه‌ها تولید می‌کند. | تولید داده‌های متنوع، کاهش Overfitting | ممکن است داده مصنوعی نامعتبر تولید کند | ✅ مناسب، مخصوصاً برای شروع مدل‌سازی و مقایسه |
| 4 | Borderline-SMOTE | Oversampling | فقط اطراف مرز کلاس‌ها نمونه‌سازی می‌کند. | تمرکز بر نقاط بحرانی، افزایش دقت | حساس به نویز | ✅ مناسب، به‌ویژه برای مدل‌های دقیق‌تر |
| 5 | ADASYN | Oversampling | در نواحی سخت‌تر، داده بیشتری تولید می‌کند. | تطبیق‌پذیر، تنوع بالا | ممکن است داده غیرواقعی تولید شود | ✅ مناسب برای مدل‌های پیچیده‌تر |
| 6 | SMOTE-ENN | Hybrid | پس از SMOTE، نمونه‌های نویزی را با ENN حذف می‌کند. | حذف نویز، داده تمیزتر | پیچیده‌تر و زمان‌بر | ✅ بسیار مناسب، مخصوصاً اگر دقت مهم باشد |
| 7 | SMOTE-Tomek | Hybrid | SMOTE + حذف نمونه‌های متداخل با Tomek Links | حذف همپوشانی کلاس‌ها | پردازش بیشتر نیاز دارد | ✅ مناسب برای حذف داده‌های مبهم و مرزی |
| 8 | KMeans-SMOTE | Oversampling هوشمند | با خوشه‌بندی قبل از SMOTE، داده متعادل و متنوع‌تری تولید می‌کند. | متنوع، کاهش Overfitting | نیاز به تنظیم دقیق K | ✅✅ بسیار مناسب برای دیتاست بزرگ و واقعی تو |


</div>

📌 **راهنمای علامت‌ها:**  
- ✅ پیشنهاد می‌شود  
- ✅✅ کاملاً توصیه می‌شود  
- ⚠️ قابل استفاده ولی نه بهترین انتخاب  
- ❌ مناسب نیست برای دیتاست تو

---


### 📊 دسته‌بندی روش‌های مدل‌سازی در کشف تقلب کارت اعتباری (CCFD)

<div dir="rtl">
  
| 🔢 ردیف | 🧠 دسته روش              | 🔤 عنوان انگلیسی               | 🧪 الگوریتم‌ها                                                                 |
|--------|---------------------------|--------------------------------|--------------------------------------------------------------------------------|
| 1️⃣     | یادگیری ماشین             | Machine Learning               | Decision Tree, Logistic Regression, SVM, Naive Bayes, KNN                      |
| 2️⃣     | یادگیری انسمبل            | Ensemble Learning              | Random Forest, AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost        |
| 3️⃣     | یادگیری عمیق             | Deep Learning                  | MLP, CNN, RNN, LSTM, Autoencoder                                               |
| 4️⃣     | یادگیری ترکیبی           | Hybrid Learning                | ML + DL، Rule-Based + ML، Stacking، Blending                                   |
| 5️⃣     | مدل‌های گرافی             | Graph-Based Models             | GCN، GAT، GraphSAGE، Graph Attention Network، GATv2                            |
| 6️⃣     | کشف ناهنجاری             | Anomaly Detection              | Isolation Forest، One-Class SVM، LOF، Autoencoder (Unsupervised)              |
| 7️⃣     | یادگیری نیمه‌نظارتی       | Semi-Supervised Learning       | Label Propagation، Self-Training، Semi-Supervised SVM، Co-Training             |
| 8️⃣     | یادگیری تقویتی            | Reinforcement Learning         | Q-Learning، Deep Q-Network (DQN)، SARSA، Policy Gradient                      |
| 9️⃣     | یادگیری بدون نظارت        | Unsupervised Learning          | KMeans، DBSCAN، PCA، Autoencoder (Unsupervised)                               |

</div>

---

### ✳️ جدول موضوعات پایان‌نامه بر اساس دسته‌بندی روش‌های مدل‌سازی در کشف تقلب کارت‌های اعتباری 💳🧠📊

<div dir="rtl">

| 🔢 ردیف | 🎓📘 عنوان پیشنهادی پایان‌نامه (فارسی / English) |
|--------|---------------------------------------------------|
| 1️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از تکنیک‌های متعادل‌سازی داده و الگوریتم‌های یادگیری ماشین  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Data Balancing Techniques and Classical Machine Learning Algorithms* |
| 2️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از مدل‌های انسمبل و تکنیک‌های متعادل‌سازی داده  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Ensemble Models and Data Balancing Techniques* |
| 3️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از الگوریتم‌های یادگیری عمیق و رویکردهای مقابله با عدم‌توازن داده  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Deep Learning Algorithms and Imbalanced Data Handling Approaches* |
| 4️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از مدل‌های ترکیبی و تکنیک‌های متعادل‌سازی داده  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Hybrid Models and Data Balancing Techniques* |
| 5️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از مدل‌های مبتنی بر گراف و روش‌های مقابله با عدم‌توازن داده  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Graph-Based Models and Imbalanced Data Handling Techniques* |
| 6️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از روش‌های کشف ناهنجاری و تکنیک‌های متعادل‌سازی داده  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Anomaly Detection Methods and Data Balancing Techniques* |
| 7️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از یادگیری نیمه‌نظارتی و روش‌های مدیریت داده‌های نامتوازن  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Semi-Supervised Learning and Imbalanced Data Handling Techniques* |
| 8️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از یادگیری تقویتی و تکنیک‌های مقابله با داده‌های نامتوازن  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Reinforcement Learning and Imbalanced Data Handling Approaches* |
| 9️⃣ | 🎓 کشف تقلب در کارت‌های اعتباری با استفاده از الگوریتم‌های یادگیری بدون نظارت و تحلیل داده‌های نامتوازن  <br> 📘 *Credit Card Fraud Detection (CCFD) Using Unsupervised Learning Algorithms and Imbalanced Data Analysis* |


</div>


---


### ✔️ مسیر پیشنهادی نهایی برای پایان‌نامه:



### 🧠 مدل‌های انسمبل چیستند؟  
مدل‌های انسمبل مجموعه‌ای از الگوریتم‌های یادگیری ماشین هستند که به‌جای تکیه بر یک مدل واحد، چندین مدل پایه را ترکیب می‌کنند تا یک تصمیم نهایی دقیق‌تر، پایدارتر و قابل اعتمادتر ارائه دهند.  

ایده اصلی انسمبل این است که «ترکیب چند مدل ضعیف، می‌تواند منجر به یک مدل قوی‌تر شود» – درست مانند خرد جمعی.



### 💡 مزایای کلیدی انسمبل:

- **🎯 افزایش دقت:** با ترکیب چند مدل، خطاهای فردی کاهش می‌یابد.  
- **🛡️ کاهش overfitting:** به‌ویژه در داده‌های نامتوازن مثل CCFD بسیار مؤثر است.  
- **📊 پایداری بیشتر:** خروجی نهایی از چند زاویه بررسی می‌شود، نه فقط یک دیدگاه.  
- **🧪 انعطاف‌پذیری:** می‌توان مدل‌ها را از خانواده‌های مختلف ترکیب کرد (مثلاً Tree و Boost).



### 🎯 مدل‌های انسمبل زیرمجموعه کدام دسته‌اند؟  
### ⛓️ ساختار سلسله‌مراتبی:

هوش مصنوعی (AI)  
└── یادگیری ماشین (Machine Learning)  
&emsp;└── یادگیری نظارت‌شده (Supervised Learning)  
&emsp;&emsp;└── مدل‌های انسمبل (Ensemble Models)



### ✳️ چرا برای پایان‌نامه ما مناسب است؟  
در مسئله کشف تقلب در کارت‌های اعتباری، داده‌ها به‌شدت نامتوازن و حساس‌اند. انسمبل‌ها با قدرت ترکیب و تعمیم، می‌توانند خطاهای تشخیص را کاهش داده، دقت را بالا برده و راهکاری قوی‌تر از مدل‌های ساده ارائه کنند.


---


## 🎯 انواع مدل‌های انسمبل (Ensemble Models)

### 1️⃣ **Bagging (Bootstrap Aggregating)**

📌 روش Bagging یکی از تکنیک‌های انسمبل ساده و مؤثر در یادگیری ماشین است که هدف اصلی آن **کاهش واریانس و افزایش پایداری مدل** است. در این روش:

1️⃣ از دیتای اصلی، چندین **زیرمجموعه تصادفی با جایگزینی (Bootstrapped samples)** ساخته می‌شود.  
2️⃣ روی هر زیرمجموعه، **یک مدل مشابه از یک الگوریتم واحد** (مثلاً فقط درخت تصمیم) آموزش داده می‌شود.  
3️⃣ خروجی نهایی از طریق **رأی‌گیری اکثریت (برای طبقه‌بندی)** یا **میانگین‌گیری (برای رگرسیون)** به‌دست می‌آید.

✅ **ویژگی‌های کلیدی:**
- 🔹 همه مدل‌ها از **یک الگوریتم مشخص** استفاده می‌کنند.  
- 🔹 تنها تفاوت بین آن‌ها، **نمونه‌های آموزش متفاوت** است.  
- 🔹 انتخاب داده‌ها در هر زیرمجموعه، کاملاً **تصادفی با جایگزینی** انجام می‌شود.  
- 🔹 مدل نهایی با ترکیب خروجی‌ها، به **نتیجه‌ای دقیق‌تر، پایدارتر و مقاوم‌تر در برابر نویز** می‌رسد.

---

### 2️⃣ **Boosting**

📌 روش Boosting یکی از تکنیک‌های قدرتمند انسمبل در یادگیری ماشین است که با هدف **کاهش بایاس و افزایش دقت مدل** طراحی شده است. در این روش:

1️⃣ مدل‌های پایه (معمولاً ضعیف) به‌صورت **ترتیبی و زنجیره‌ای** آموزش داده می‌شوند.  
2️⃣ هر مدل جدید، **روی خطاهای مدل قبلی تمرکز** می‌کند و تلاش می‌کند پیش‌بینی آن‌ها را اصلاح کند.  
3️⃣ خروجی نهایی با **ترکیب وزن‌دار پیش‌بینی‌های تمام مدل‌ها** به‌دست می‌آید.

✅ **ویژگی‌های کلیدی:**
- 🔹 همه مدل‌ها از **یک الگوریتم یکسان** (مثلاً درخت تصمیم ساده) استفاده می‌کنند.  
- 🔹 مدل‌ها به‌صورت **وابسته و مرحله‌ای** آموزش می‌بینند.  
- 🔹 در هر مرحله، به **نمونه‌های اشتباه‌شده قبلی وزن بیشتری** داده می‌شود.  
- 🔹 Boosting در مواجهه با **داده‌های نامتوازن و الگوهای پیچیده** عملکرد بسیار دقیقی دارد.

***

### 3️⃣ **Stacking (Stacked Generalization)**

📌 روش Stacking یک تکنیک انسمبل پیشرفته در یادگیری ماشین است که با هدف افزایش دقت مدل نهایی، از **ویژگی‌سازی مبتنی بر خروجی چند مدل مختلف** استفاده می‌کند و آن‌ها را به یک **مدل نهایی هوشمند** می‌سپارد تا تصمیم‌گیری نهایی انجام شود.

1️⃣ چندین **مدل پایه (Base Learners)** با الگوریتم‌های متفاوت (مثل SVM، Random Forest، KNN) روی داده‌ی اصلی آموزش داده می‌شوند.  
2️⃣ خروجی‌های این مدل‌ها (پیش‌بینی یا احتمال) برای هر نمونه استخراج می‌گردد.  
3️⃣ این خروجی‌ها به‌عنوان **ویژگی‌های جدید** به یک **مدل نهایی (Meta Learner)** داده می‌شوند تا تصمیم نهایی را اتخاذ کند.

✅ **ویژگی‌های کلیدی:**
- 🔹 استفاده از مدل‌های پایه با **الگوریتم‌های متنوع و مستقل**  
- 🔹 مدل نهایی می‌تواند **هر الگوریتمی** باشد (معمولاً ساده، مثل Logistic Regression یا XGBoost)  
- 🔹 مناسب برای مسائل پیچیده و **داده‌های نامتوازن**  
- 🔹 عملکرد کلی معمولاً بهتر از هر یک از مدل‌ها به‌تنهایی است

***

### 4️⃣ **Voting (Hard Voting / Soft Voting)**

📌روش Voting یکی از روش‌های ساده و مؤثر انسمبل در یادگیری ماشین است که با ترکیب خروجی چند مدل مستقل، تصمیم نهایی را اتخاذ می‌کند. در این روش:

1️⃣ یک دیتاست مشخص، به چندین مدل یادگیری ماشین با **الگوریتم‌های متفاوت** داده می‌شود.  
2️⃣ هر مدل به‌صورت **مستقل و موازی** آموزش می‌بیند.  
3️⃣ در زمان پیش‌بینی، خروجی تمام مدل‌ها گرفته شده و **ترکیب می‌شود** تا نتیجه نهایی تولید شود.

✅ **دو نوع اصلی Voting:**
- 🧩 **Hard Voting:** انتخاب کلاسی که **بیشترین رأی** را از بین مدل‌ها کسب کرده باشد  
- 🧮 **Soft Voting:** انتخاب کلاسی که **بیشترین میانگین احتمال** را از بین مدل‌ها داشته باشد

🗳️ **نوع تصمیم‌گیری:**
- 🔷 **Hard Voting** → اکثریت رأی‌ها (کدام کلاس بیشتر رأی آورده؟)  
- 🔶 **Soft Voting** → میانگین احتمال‌ها (کدام کلاس احتمال بیشتری دارد؟)

🎯 **ویژگی‌های کلیدی:**
- 🔹 استفاده از مدل‌های متنوع (مثل Random Forest، KNN، SVM و ...)  
- 🔹 پیاده‌سازی آسان و مناسب برای شروع ترکیب مدل‌ها  
- 🔹 کاهش خطای کلی سیستم با **پوشش ضعف هر مدل توسط مدل‌های دیگر**  
- 🔹 Soft Voting معمولاً عملکرد دقیق‌تری نسبت به Hard Voting دارد

---

### 📊 مقایسه روش‌های انسمبل: Bagging، Boosting، Stacking و Voting
<div dir="rtl">


| 🧩 ویژگی / روش         | 🧺 **Bagging**                        | 🚀 **Boosting**                       | 🧠 **Stacking**                         | 🗳️ **Voting**                          |
|------------------------|---------------------------------------|---------------------------------------|-----------------------------------------|----------------------------------------|
| 🔁 **نوع آموزش**       | موازی و مستقل                         | زنجیره‌ای و وابسته                    | موازی در پایه، بعد یک مدل نهایی        | کاملاً مستقل و موازی                   |
| 🧠 **مدل‌ها**          | یک الگوریتم (تکراری)                 | یک الگوریتم (تکراری)                  | الگوریتم‌های مختلف                      | الگوریتم‌های مختلف                     |
| 🎯 **نحوه ترکیب**      | میانگین / رأی‌گیری                    | ترکیب وزن‌دار بر اساس خطای قبلی       | مدل نهایی (Meta Learner)                | رأی‌گیری (Hard/Soft)                   |
| 🎯 **هدف اصلی**       | کاهش واریانس (Variance)              | کاهش بایاس (Bias) و افزایش دقت        | بهینه‌سازی ترکیبی از چند مدل پایه      | افزایش پایداری و دقت                  |
| 🧪 **پیچیدگی پیاده‌سازی** | متوسط                               | بالا                                   | نسبتاً بالا                              | ساده                                   |
| 💥 **نکته مهم**         | مدل‌ها از یک نوع هستن                | آموزش مرحله‌به‌مرحله روی خطاها         | خروجی مدل‌های پایه ورودی مدل نهایی می‌شن | فقط خروجی نهایی هر مدل بررسی می‌شه     |

</div>


---

## ✅ مناسب‌ترین الگوریتم **انسمبل** برای دیتاست تو:

### 📦 وقتی داده‌هات از نوع **متنی یا دسته‌ای (Categorical)** باشن، مثل:

- `merchant_name`  
- `category`  
- `gender`  
- `card_type`  
- `job`  
- `city`  
- و حتی `datetime` (اگه به روز و ساعت تبدیل بشه)


### ❗ چرا خیلی از الگوریتم‌ها با این نوع داده‌ها مشکل دارن؟

- باید به عدد تبدیلشون کنی (One-Hot یا Label Encoding)  
- بعد از تبدیل، دقت افت می‌کنه  
- یا روی ویژگی‌های پرتکرار، overfitting می‌زنن 😒


## 🥇 **CatBoost (Boosting-based Ensemble)**


### 🎯 چرا CatBoost از همه بهتره برای تو؟

<div dir="rtl">
  
| 📝 دلیل | 📌 توضیح |
|--------|---------|
| 🎯 پشتیبانی از ویژگی‌های دسته‌ای | بدون نیاز به one-hot یا label encoding (`gender`, `job`, `category`, ...) |
| 🔍 قدرت در کشف روابط پیچیده | عالی برای ویژگی‌های متنی و غیرخطی |
| ⚖️ مقاومت در برابر overfitting | به کمک تکنیک Ordered Boosting |
| 📈 مناسب برای داده‌های نامتوازن | بارها در مقالات CCFD تأیید شده |
| 🔧 تنظیمات راحت‌تر از XGBoost و LightGBM | مناسب برای پروژه‌ی ارشد با زمان محدود |
| 🆕 کمتر تکراری تو مقالات | نوآوری پایان‌نامه‌ات رو تضمین می‌کنه |
| 📊 پشتیبانی از تفسیر با SHAP | فوق‌العاده برای ارائه و مقاله‌نویسی |

<div dir="rtl">
  
---

### 🧐 سایر الگوریتم‌ها در برابر داده‌های متنی:

<div dir="rtl">


| الگوریتم                    | ✅ با داده‌های دسته‌ای راحت کار می‌کنه؟ | 🔁 نیاز به Encoding |
|-----------------------------|-----------------------------------------|----------------------|
| **CatBoost**                | ✅ بله، خودش انجام می‌ده                | ❌ نداره              |
| **LightGBM**                | ⚠️ بله، ولی باید Label Encoding بدی     | ✅ داره               |
| **XGBoost**                 | ❌ نه، با categorical سازگار نیست      | ✅ باید تبدیل شه      |
| **Random Forest / Logistic / KNN** | ❌ نه                              | ✅ باید کامل تبدیل شه |

</div>

---

### ✅ پیشنهاد نهایی:

> 🎓 برای پایان‌نامه‌ی تو، **CatBoost بهترین انتخاب انسمبلیه**.  
> چون داده‌هات پر از **ویژگی متنی و دسته‌ایه**،  
> CatBoost انتخاب شماره ۱ توئه ✨  
> هم از نظر **دقت**، هم **نوآوری**، هم **سادگی اجرا**، و هم **سازگاری کامل با نوع داده‌هات**.

---


## ✅ **رتبه‌بندی نهایی Boosting Algorithms برای پروژه**

### 📌 شرایطی که در نظر گرفته‌ایم:

| 🎯 شرط | 📍 در نظر گرفته شده؟ |
|--------|------------------------|
| داده‌ها نامتوازن هستن | ✅ بله (تعداد تقلب‌ها بسیار کمتر از نرمال‌هاست) |
| ستون‌ها معنای مشخص دارن (مثل gender, job...) | ✅ بله |
| ویژگی‌های دسته‌ای (categorical) زیاد داریم | ✅ بله |
| می‌خوای Feature Engineering و Selection انجام بدی | ✅ بله |
| هدفت دقت بالا، تکراری نبودن، اجرا در Colab، و مقاله هست | ✅ دقیقاً همین |



### 🏆 رتبه‌بندی الگوریتم‌ها:

| 🥇 رتبه | 🚀 الگوریتم | 💬 چرا در این رتبه قرار گرفته؟ |
|--------|-------------|-------------------------------|
| 🥇 1 | **CatBoost** | 🔹 بهترین عملکرد برای داده‌های categorical بدون نیاز به encoding <br> 🔹 کمترین نیاز به تنظیمات (hyperparameter tuning) <br> 🔹 پشتیبانی قوی از missing values <br> 🔹 بسیار مناسب برای دیتاست اروپا با ستون‌های نام‌دار <br> 🔹 **کمتر کار شده → مناسب برای نوآوری** |
| 🥈 2 | **LightGBM** | 🔹 بسیار سریع و کم‌مصرف <br> 🔹 دقت بالا در داده‌های بزرگ <br> 🔹 از categorical پشتیبانی می‌کنه ولی باید تبدیل کنی (label encoding) <br> 🔹 اگر Feature Engineering قوی انجام بدی، نتیجه عالی می‌ده <br> 🔸 نسبت به CatBoost کمی تکراری‌تر شده |
| 🥉 3 | **XGBoost** | 🔹 دقت بالا و الگوریتمی امتحان‌شده <br> 🔹 کنترل بالا روی پارامترها <br> 🔸 نیاز به تنظیمات زیاد <br> 🔸 پشتیبانی ضعیف‌تر برای داده‌های دسته‌ای <br> 🔸 بسیار **تکراری** در مقالات تقلب؛ فقط در صورت نوآوری قابل استفاده است |



### ✨ جمع‌بندی نهایی:

| 🎯 هدف تو | 🧠 الگوریتم پیشنهادی |
|-----------|------------------------|
| دقت + نوآوری + اجرا با داده‌های واقعی (نام‌دار) | ✅ **CatBoost** |
| اجرا سریع + نتیجه خوب + امکان ترکیب با مهندسی ویژگی | ✅ **LightGBM** |
| فقط در صورت نوآوری بسیار قوی | ⚠️ **XGBoost** |

---


## ✅ **رتبه‌بندی نهایی روش‌های متعادل‌سازی برای پروژه**

### 📌 شرایط در نظر گرفته‌شده برای رتبه‌بندی:

| 🎯 شرط | 🟢 بررسی‌شده |
|-------------------------------|------------------|
| داده‌ها **نامتوازن و واقعی** هستن | ✅ بله |
| ویژگی‌ها **نام‌دار و معنایی** هستن | ✅ بله |
| **دقت مدل بسیار مهم**ه | ✅ کاملاً |
| **اجرای ساده در Colab و Python** | ✅ باید عملی باشه |
| می‌خوای با **مدل‌های Boosting** ترکیب کنی | ✅ بله |
| می‌خوای مقالات تکراری نباشه | ✅ خیلی مهمه |



### 🏆 رتبه‌بندی روش‌های متعادل‌سازی:

| 🥇 رتبه | ⚖️ روش متعادل‌سازی | 💡 چرا این رتبه؟ |
|--------|---------------------|------------------|
| 🥇 1 | **KMeans-SMOTE** | 🔹 داده‌های تقلب اغلب خوشه‌ای هستن؛ این روش خوشه‌بندی قبل از نمونه‌سازی انجام می‌ده <br> 🔹 تولید داده واقعی‌تر نسبت به SMOTE ساده <br> 🔹 بسیار مناسب برای داده‌های نام‌دار با ساختار رفتاری پیچیده <br> 🔸 در مقالات کمتر استفاده شده → نوآورانه‌تر |
| 🥈 2 | **SMOTE-ENN** | 🔹 ترکیب تولید نمونه جدید (SMOTE) با حذف نویز (ENN) <br> 🔹 دقت بالا در کنار کاهش overfitting <br> 🔹 عملکرد قوی با الگوریتم‌های Boosting <br> 🔸 نسبتاً تکراری در مقالات |
| 🥉 3 | **ADASYN** | 🔹 نمونه‌سازی هدفمند: تمرکز روی نقاط سخت‌تر برای طبقه‌بندی <br> 🔹 قابل اجرا و مفید با مدل‌های LightGBM و CatBoost <br> 🔸 گاهی باعث تولید داده‌های synthetic عجیب می‌شه |
| 4️⃣ | **Borderline-SMOTE** | 🔹 تمرکز روی نقاط مرزی بین کلاس‌ها <br> 🔸 اما اگر داده noisy باشه، ممکنه اشتباه تقویت کنه |
| 5️⃣ | **SMOTE ساده** | 🔹 ساده‌ترین و رایج‌ترین <br> 🔸 در مقالات زیاد تکرار شده <br> 🔸 گاهی باعث overfitting یا تولید داده‌های غیرواقعی می‌شه |
| 6️⃣ | **SMOTE-Tomek Links** | 🔹 حذف نقاط تکراری یا گیج‌کننده <br> 🔸 اجرای پیچیده‌تر و در برخی موارد کاهش دقت نهایی |



### ✨ جمع‌بندی پیشنهادی:

| 🎯 هدف تو | 🧩 روش پیشنهادی |
|-----------|------------------|
| نوآوری + عملکرد عالی + مقاله | ✅ **KMeans-SMOTE** |
| تعادل بین دقت بالا و حذف نویز | ✅ **SMOTE-ENN** |
| سرعت و سادگی + تنوع | ✅ **ADASYN** فقط در صورتی که KMeans-SMOTE اجرا نشد |


---

<h2 dir="rtl" align="right">🧩 Feature Processing یعنی چی؟</h2>

<p dir="rtl" align="right">
Feature Processing یک اصطلاح <strong>کلی و جامع</strong>ه که شامل تمام عملیاتیه که روی ویژگی‌ها (Features) انجام می‌دیم تا اونا رو برای مدل آماده کنیم.
</p>


یعنی:

> 🎯 **Feature Processing = Feature Engineering + Feature Selection + Normalization + Encoding + ...**


### 📦 اجزای مهم Feature Processing:

| 🔢 مرحله | 💡 توضیح |
|---------|----------|
| 🛠 **Feature Engineering** | ساخت یا استخراج ویژگی‌های جدید از داده‌های خام |
| ✂️ **Feature Selection** | انتخاب بهترین ویژگی‌ها و حذف اضافی‌ها |
| 📐 **Normalization / Scaling** | نرمال‌سازی داده‌های عددی (مثلاً با MinMax یا StandardScaler) |
| 🏷 **Encoding** | تبدیل ویژگی‌های دسته‌ای به عددی (مثل One-Hot, Label Encoding) |
| 💫 **Transformation** | تبدیل‌های لگاریتمی، باینری، بُعدکاهش و... برای بهبود توزیع داده‌ها |

---


دمت گرم بابت این همه نظم، دقت و فکر پشت انتخابات 🔥  
الان کاملاً موقعشه که برات این بخش رو هم ✨ حرفه‌ای و پرزنت‌پذیر ✨ کنم —  
کاملاً با رعایت این موارد:

✅ بدون هیچ تغییر در محتوای اصلی  
✅ فقط زیباسازی، جدول‌کشی حرفه‌ای، و تراز Markdown  
✅ آماده برای GitHub، PDF، دفاع، یا مقاله

---

## 🧪💎 **بهترین ترکیبات ممکن (کاملاً قابل دفاع، اجرا، و نوآورانه)**

### ✅ جدول نهایی ترکیب‌های پیشنهادی برای کشف تقلب در کارت‌های اعتباری (CCFD):

| 🔢 ردیف | 🎯 الگوریتم یادگیری | ⚖️ روش متعادل‌سازی داده | 🧠 Feature Processing |
|--------|---------------------|---------------------------|------------------------|
| 1️⃣ | **CatBoost**         | **KMeans-SMOTE**          | ✅                    |
| 2️⃣ | **CatBoost**         | **SMOTE-ENN**             | ✅                    |
| 3️⃣ | **LightGBM**         | **KMeans-SMOTE**          | ✅                    |
| 4️⃣ | **CatBoost**         | **ADASYN**                | ✅                    |
| 5️⃣ | **XGBoost**          | **KMeans-SMOTE**          | ✅                    |



### 📝 جمع‌بندی نهایی:

| 🎯 پیشنهاد نهایی | 💎 ترکیب نهایی پیشنهادی |
|------------------|----------------------------|
| اگر هدفت **نوآوری + مقاله‌پذیری + اجراپذیری بالا** باشه | ✅ **KMeans-SMOTE + CatBoost + Feature Processing** |
| اگر اولویتت **دقت و ساده بودن اجراست** | ✅ **SMOTE-ENN + CatBoost + Feature Processing** |

---
